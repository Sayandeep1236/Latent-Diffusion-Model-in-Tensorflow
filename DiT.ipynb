{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "9643448e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "import math\n",
    "import tensorflow_addons as tfa\n",
    "import time\n",
    "import os\n",
    "\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input ,Conv2D, Conv2DTranspose, LeakyReLU, Activation, Concatenate, Dropout, BatchNormalization, LeakyReLU, Dense\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "573e6035",
   "metadata": {},
   "outputs": [],
   "source": [
    "from  VAE_trained import build_model, meanvar, reparameterize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "b5186cad",
   "metadata": {},
   "outputs": [],
   "source": [
    "img_size = 128\n",
    "new_img_size = 32\n",
    "img_channels = 3\n",
    "\n",
    "path = 'C:\\\\Users\\\\sayan\\\\Desktop\\\\anime_pics\\\\128x128_'\n",
    "train_paths = [os.path.join(path, img) for img in os.listdir(path)]\n",
    "\n",
    "train_image = list(train_paths)\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices(train_image)\n",
    "\n",
    "def load_images(path):\n",
    "    img = tf.io.read_file(path)\n",
    "    img = tf.image.decode_jpeg(img)\n",
    "    img = tf.image.resize(img, [img_size, img_size])\n",
    "    img = (img / 127.5) - 1\n",
    "\n",
    "    return img\n",
    "\n",
    "train_dataset = train_dataset.map(lambda x: load_images(x))\n",
    "train_dataset = train_dataset.shuffle(buffer_size=len(train_image)).batch(16).prefetch(tf.data.AUTOTUNE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "b2ea2266",
   "metadata": {},
   "outputs": [],
   "source": [
    "depth = 3\n",
    "\n",
    "channel_multiplier = [i for i in range(depth)]\n",
    "filters = [64 * 2**mult for mult in channel_multiplier]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "7158f811",
   "metadata": {},
   "outputs": [],
   "source": [
    "enc, dec = build_model(img_size, img_channels, filters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "4feb0c6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "enc.load_weights('LDM_encoder.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "0239febf",
   "metadata": {},
   "outputs": [],
   "source": [
    "dec.load_weights('LDM_decoder.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "f4bcd33f",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = enc\n",
    "decoder = dec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "76cac5dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "patch_size = 4\n",
    "num_layers = 12\n",
    "hidden_size = 384\n",
    "num_heads = 6\n",
    "units = 64\n",
    "temb_dim = 384"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "f0de8772",
   "metadata": {},
   "outputs": [],
   "source": [
    "kernel_init = keras.initializers.RandomNormal(mean=0.0, stddev=0.05, seed=32)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "1901df28",
   "metadata": {},
   "outputs": [],
   "source": [
    "def time_embedding(timesteps, dim):\n",
    "        half_dim = dim // 2\n",
    "        emb = math.log(10000) / (half_dim - 1)                    # a = 2 * ln(10000)/ d\n",
    "        emb = tf.exp(tf.range(half_dim, dtype=tf.float32) * -emb) # e^(-a * i) where i runs from 0 to half_dim-1\n",
    "        time = tf.cast(timesteps, dtype=tf.float32)               # pos\n",
    "        emb = time[:, None] * emb[None, :]                        # pos * e^(-a * i)\n",
    "        emb = tf.concat([tf.sin(emb), tf.cos(emb)], axis=-1)      # [sin(pos * e^(-2 i ln(10000)/ d)) , cos(pos * e^(-2 i ln(10000)/ d)] where i runs from 0 to half_dim-1\n",
    "        return emb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "7841a336",
   "metadata": {},
   "outputs": [],
   "source": [
    "def TimeMLP(units, activation_fn=keras.activations.swish):\n",
    "    def apply(inputs):\n",
    "        time_emb = layers.Dense(units, activation=activation_fn, kernel_initializer=kernel_init)(inputs)\n",
    "        time_emb = layers.Dense(units, kernel_initializer=kernel_init)(time_emb)\n",
    "        return time_emb\n",
    "    return apply"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "9bffc1bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SineCosinePositionalEmbedding2D(tf.keras.layers.Layer):\n",
    "    def __init__(self, embed_dim, **kwargs):\n",
    "        super(SineCosinePositionalEmbedding2D, self).__init__(**kwargs)\n",
    "        assert embed_dim % 4 == 0, \"Embed dim must be divisible by 4\"\n",
    "        self.embed_dim = embed_dim\n",
    "        self.d_quarter = embed_dim // 4\n",
    "\n",
    "    def call(self, height, width):\n",
    "\n",
    "        H = height\n",
    "        W = width\n",
    "\n",
    "        # Create position indices\n",
    "        rows = tf.cast(tf.range(H), tf.float32)[:, tf.newaxis]  # [H, 1]\n",
    "        cols = tf.cast(tf.range(W), tf.float32)[:, tf.newaxis]  # [W, 1]\n",
    "        div_term = tf.exp(tf.range(self.d_quarter, dtype=tf.float32) * -(tf.math.log(10000.0) / self.d_quarter))  # [D/4]\n",
    "\n",
    "        # Sin/cos for rows\n",
    "        sin_row = tf.sin(rows * div_term)                       # [H, D/4]\n",
    "        cos_row = tf.cos(rows * div_term)                       # [H, D/4]\n",
    "        sin_col = tf.sin(cols * div_term)                       # [W, D/4]\n",
    "        cos_col = tf.cos(cols * div_term)                       # [W, D/4]\n",
    "\n",
    "        # Expand to 3D and tile\n",
    "        sin_row = tf.tile(sin_row[:, tf.newaxis, :], [1, W, 1])  # [H, W, D/4]\n",
    "        cos_row = tf.tile(cos_row[:, tf.newaxis, :], [1, W, 1])\n",
    "        sin_col = tf.tile(sin_col[tf.newaxis, :, :], [H, 1, 1])\n",
    "        cos_col = tf.tile(cos_col[tf.newaxis, :, :], [H, 1, 1])\n",
    "\n",
    "        # Concatenate and reshape\n",
    "        pos_emb = tf.concat([sin_row, cos_row, sin_col, cos_col], axis=-1)  # [H, W, D]\n",
    "        pos_emb = tf.reshape(pos_emb, [H *  W, self.embed_dim])              # [H*W, D]\n",
    "\n",
    "        return pos_emb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "47b9e233",
   "metadata": {},
   "outputs": [],
   "source": [
    "def PatchEmbedding(tensor, patch_h, patch_w, dims = hidden_size):\n",
    "  \n",
    "  batch_size, height, width, channels = tf.shape(tensor)[0], tf.shape(tensor)[1], tf.shape(tensor)[2], tf.shape(tensor)[3]\n",
    "  h_patches, w_patches = height // patch_h, width // patch_w\n",
    "  tensor = tf.reshape(tensor, (batch_size, h_patches * w_patches, patch_h , patch_w , channels))\n",
    "  tensor = tf.reshape(tensor, (batch_size, h_patches * w_patches, patch_h * patch_w * channels))\n",
    "\n",
    "  tensor = layers.Dense(dims)(tensor)\n",
    "  \n",
    "  #temb = get_2d_sincos_pos_embed(dims, h_patches, w_patches)\n",
    "  temb = SineCosinePositionalEmbedding2D(dims)(h_patches, w_patches)\n",
    "  temb = tf.expand_dims(temb, axis=0)\n",
    "  tensor = tensor + temb\n",
    "  \n",
    "  return tensor\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50d86dbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class AttentionBlock(layers.Layer):\n",
    "\n",
    "    def __init__(self, heads, units,**kwargs):\n",
    "        super(AttentionBlock,self).__init__()\n",
    "        self.units = units\n",
    "        self.num_heads = heads\n",
    "        assert self.units%self.num_heads == 0, 'nummber of heads is incompatible with hidden dims size'\n",
    "        self.units_per_heads = self.units//self.num_heads\n",
    "        \n",
    "        #self.norm = layers.LayerNormalization()\n",
    "        self.query_ = layers.Dense(units)\n",
    "        self.key_ = layers.Dense(units)\n",
    "        self.value_ = layers.Dense(units)\n",
    "        self.proj = layers.Dense(units)\n",
    "\n",
    "    def split_heads(self, input, batch_size):\n",
    "        input = tf.reshape(input, (batch_size, -1, self.num_heads, self.units_per_heads))\n",
    "        return tf.transpose(input, perm=[0, 2, 1, 3])\n",
    "    \n",
    "    def dot_prod(self,x):\n",
    "      q_i, k_i, v_i = x\n",
    "      scale = tf.cast(self.units_per_heads, q_i.dtype)**-0.5\n",
    "      dot   = tf.matmul(q_i, k_i, transpose_b=True) * scale\n",
    "      w     = tf.nn.softmax(dot, axis=-1)\n",
    "      return tf.matmul(w, v_i)\n",
    "\n",
    "\n",
    "    \n",
    "    def call (self, inputs):\n",
    "        #inputs = self.norm(inputs)\n",
    "        batch_size = tf.shape(inputs)[0]\n",
    "\n",
    "        q = self.query_(inputs)\n",
    "        k = self.key_(inputs)\n",
    "        v = self.value_(inputs)\n",
    "\n",
    "        q = self.split_heads(q, batch_size)\n",
    "        k = self.split_heads(k, batch_size)\n",
    "        v = self.split_heads(v, batch_size)\n",
    "        \n",
    "        attn = tf.map_fn(self.dot_prod, (q, k, v), fn_output_signature=q.dtype)\n",
    "\n",
    "        attn = tf.transpose(attn, perm=[0, 2, 1, 3])\n",
    "        attn = tf.reshape(attn, (batch_size, -1, self.units))\n",
    "        attn = self.proj(attn)\n",
    "\n",
    "        return  attn\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "baed146a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "577e74c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def MLP(attn_score, units = hidden_size):\n",
    "    #x = layers.LayerNormalization()(attn_score)\n",
    "    x = layers.Dense(3*units)(attn_score)\n",
    "    x = keras.activations.gelu(x, approximate='tanh')\n",
    "    x = layers.Dense(units)(x)\n",
    "    return x "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "9fcf1d2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def transformer_block(x, t, heads = num_heads, hidden_size = hidden_size):\n",
    "\n",
    "  norm_attn = layers.LayerNormalization(center = False, scale = False, trainable = False)\n",
    "  norm_mlp = layers.LayerNormalization(center = False, scale = False, trainable = False)\n",
    "\n",
    "  ada_norm = layers.Dense(6* hidden_size, kernel_initializer='zeros', bias_initializer = 'zeros', activation = tf.keras.activations.swish)(t)\n",
    "\n",
    "  (pre_attn_scale, pre_attn_shift, post_attn_scale, pre_mlp_scale, pre_mlp_shift, post_mlp_scale) = tf.split(ada_norm, num_or_size_splits=6, axis=-1)\n",
    "\n",
    "\n",
    "  out = norm_attn(x) * (1 + pre_attn_scale) + pre_attn_shift\n",
    "  out = out + post_attn_scale* AttentionBlock(heads, hidden_size)(out)\n",
    "\n",
    "  out = norm_mlp(out) * (1 + pre_mlp_scale) + pre_mlp_shift\n",
    "  attention_out = out + post_mlp_scale * MLP(out, hidden_size)\n",
    "\n",
    "\n",
    "  return attention_out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "d459bb5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def transformer(num_layers = num_layers, patch_height = patch_size, patch_width = patch_size,units = units, hidden_size = hidden_size, img_size = new_img_size, img_channels = img_channels):\n",
    "    \n",
    "    input = layers.Input(shape=(img_size, img_size, img_channels), name=\"image_input\")\n",
    "    time_input = keras.Input(shape=(), dtype=tf.int64, name=\"time_input\")\n",
    "\n",
    "    num_patches_h = img_size//patch_height\n",
    "    num_patches_w = img_size//patch_width\n",
    "\n",
    "    x = PatchEmbedding(input, patch_height, patch_width)\n",
    "\n",
    "    \n",
    "    t = time_embedding(time_input, units)\n",
    "    t = TimeMLP(hidden_size)(t)[:, None, :]\n",
    "\n",
    "    for _ in range(num_layers):\n",
    "        x = transformer_block(x, t)\n",
    "\n",
    "    x = layers.Dense(patch_height*patch_width*img_channels)(x)\n",
    "\n",
    "    x = layers.Reshape((patch_height*num_patches_h, patch_width*num_patches_w, img_channels))(x)\n",
    "\n",
    "    return keras.Model([input,time_input], x, name= 'DiT')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "dc0f0b3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "diffusion_model = transformer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "f34269f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "timesteps = 1000\n",
    "beta_start = 0.0001\n",
    "beta_end = 0.02\n",
    "betas = tf.linspace(beta_start, beta_end, timesteps)                   #beta schedule\n",
    "\n",
    "alphas = 1 - betas\n",
    "alphas_cumprod = tf.math.cumprod(alphas)                               # cumulative product of alpha\n",
    "alphas_cumprod_prev = tf.concat([[1.0], alphas_cumprod[:-1]], axis=0)  # previous cumulative product\n",
    "\n",
    "sqrt_alphas_cumprod = tf.sqrt(alphas_cumprod)\n",
    "sqrt_one_minus_alphas_cumprod = tf.sqrt(1.0 - alphas_cumprod)\n",
    "sqrt_reciprocal_alphas_cumprod = tf.sqrt(1.0 / alphas_cumprod)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "eadb7c13",
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_diffusion(x0, t, func_sqrt_alphas_cumprod = sqrt_alphas_cumprod,func_sqrt_one_minus_alphas_cumprod = sqrt_one_minus_alphas_cumprod):\n",
    "        ''' given x0 and t, returns xt and noise '''\n",
    "        \n",
    "        noise = tf.random.normal(tf.shape(x0))\n",
    "        \n",
    "        sqrt_alphas_cumprod_t = tf.gather(func_sqrt_alphas_cumprod, t)\n",
    "        sqrt_one_minus_alphas_cumprod_t = tf.gather(func_sqrt_one_minus_alphas_cumprod, t)\n",
    "        \n",
    "        # reshape for broadcasting\n",
    "        sqrt_alphas_cumprod_t = tf.reshape(sqrt_alphas_cumprod_t, [-1, 1, 1, 1])\n",
    "        sqrt_one_minus_alphas_cumprod_t = tf.reshape(sqrt_one_minus_alphas_cumprod_t, [-1, 1, 1, 1])\n",
    "   \n",
    "        xt = sqrt_alphas_cumprod_t * x0 + sqrt_one_minus_alphas_cumprod_t * noise #forward process    \n",
    "        \n",
    "        return xt, noise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "d2738bed",
   "metadata": {},
   "outputs": [],
   "source": [
    "def reverse_diffusion( xt, t, predicted_noise, func_alphas = alphas, func_alphas_cumprod = alphas_cumprod, func_alphas_cumprod_prev = alphas_cumprod_prev, func_betas = betas):\n",
    "        ''' given xt, t, and original noise added/ predicted noise, returns x_prev ''' \n",
    "\n",
    "        alpha_t = tf.gather(func_alphas, t)\n",
    "        alpha_cumprod_t = tf.gather(func_alphas_cumprod, t)\n",
    "        alpha_cumprod_prev_t = tf.gather(func_alphas_cumprod_prev, t)\n",
    "        beta_t = tf.gather(func_betas, t)\n",
    "        \n",
    "        # Reshape for broadcasting\n",
    "        alpha_t = tf.reshape(alpha_t, [-1, 1, 1, 1])\n",
    "        alpha_cumprod_t = tf.reshape(alpha_cumprod_t, [-1, 1, 1, 1])\n",
    "        alpha_cumprod_prev_t = tf.reshape(alpha_cumprod_prev_t, [-1, 1, 1, 1])\n",
    "        beta_t = tf.reshape(beta_t, [-1, 1, 1, 1])\n",
    "        \n",
    "        mean = (1.0 / tf.sqrt(alpha_t)) * (xt - (beta_t / tf.sqrt(1.0 - alpha_cumprod_t)) * predicted_noise)\n",
    "        var = beta_t * (1.0 - alpha_cumprod_prev_t) / (1.0 - alpha_cumprod_t)\n",
    "       \n",
    "        noise = tf.random.normal(tf.shape(xt))\n",
    "\n",
    "        nonzero_mask = tf.reshape(tf.cast(t != 0, tf.float32), [-1, 1, 1, 1]) #if first step, no noise is added\n",
    "        \n",
    "        x_prev = mean + nonzero_mask * tf.sqrt(var) * noise\n",
    "        return x_prev"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "91ff3761",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample(func_model, num_steps=None, batch_size=1):\n",
    "        if num_steps is None:\n",
    "            num_steps = 1000\n",
    "\n",
    "        xt = tf.random.normal((batch_size, new_img_size, new_img_size, img_channels)) #last step of reverse diffusion starts with random noise\n",
    "        \n",
    "        # Reverse diffusion\n",
    "        for i in reversed(range(num_steps)):\n",
    "            t = tf.fill([batch_size], i)\n",
    "\n",
    "            '''for each step, we predict the original noise added to x0 to give xt, and use it to get x_prev\n",
    "            jumping directly from xt to x0 given predicted noise will make it not a meaningful image, we need to go step by step'''\n",
    "            predicted_noise = func_model([xt, t], training=False) \n",
    "            xt = reverse_diffusion(xt, t, predicted_noise)\n",
    "            \n",
    "        return xt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "e6200599",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_step_(x_batch):\n",
    "        batch_size = tf.shape(x_batch)[0]\n",
    "        t = tf.random.uniform([batch_size], 0, timesteps, dtype=tf.int32) # sample random timesteps\n",
    "        with tf.GradientTape() as tape:\n",
    "        \n",
    "            xt, noise = forward_diffusion(x_batch, t)# forward diffusion\n",
    "            predicted_noise = diffusion_model([xt, t], training=True)# Predict noise\n",
    "            loss = tf.reduce_mean(tf.square(noise - predicted_noise))# compute loss (MSE between actual and predicted noise)\n",
    "        \n",
    "        gradients = tape.gradient(loss, diffusion_model.trainable_variables)\n",
    "        return loss, gradients #gradients and loss\n",
    "\n",
    "def train_diffusion_model(diffusion_model, dataset, epochs=100, learning_rate=1e-4):\n",
    "    optimizer = keras.optimizers.Adam(learning_rate=learning_rate)\n",
    "\n",
    "    diffusion_model.compile(optimizer=optimizer)\n",
    "    \n",
    "    @tf.function\n",
    "    def train_step(x_batch):\n",
    "        loss, gradients = train_step_(x_batch)\n",
    "        optimizer.apply_gradients(zip(gradients, diffusion_model.trainable_variables))\n",
    "        return loss\n",
    "    \n",
    "    #training loop\n",
    "    for epoch in range(epochs):\n",
    "        epoch_loss = 0\n",
    "        num_batches = 0\n",
    "        \n",
    "        for batch in dataset:\n",
    "            mean, var = meanvar(batch, enc, training=False)\n",
    "            batch_ = reparameterize(mean, var)\n",
    "            loss = train_step(batch_)\n",
    "            epoch_loss += loss\n",
    "            num_batches += 1\n",
    "        \n",
    "        avg_loss = epoch_loss / num_batches\n",
    "        print(f\"Epoch {epoch + 1}/{epochs}, Loss: {avg_loss:.4f}\")\n",
    "        \n",
    "        # Generate samples every 10 epochs\n",
    "        if (epoch + 1) % 10 == 0:\n",
    "            samples = sample(diffusion_model,batch_size=4)\n",
    "            decoded_samples = decoder(samples, training=False)\n",
    "            \n",
    "            \n",
    "            # Plot samples\n",
    "            fig, axes = plt.subplots(1, 4, figsize=(12, 3))\n",
    "            for i in range(4):\n",
    "                axes[i].imshow(decoded_samples[i])\n",
    "                axes[i].axis('off')\n",
    "\n",
    "            plt.title(f\"Generated samples at epoch {epoch + 1}\")\n",
    "            plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "de9b4eea",
   "metadata": {},
   "outputs": [],
   "source": [
    "diffusion_model.load_weights('DiT.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "f0bc3889",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[56], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mtrain_diffusion_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdiffusion_model\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_dataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m100000\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[54], line 32\u001b[0m, in \u001b[0;36mtrain_diffusion_model\u001b[1;34m(diffusion_model, dataset, epochs, learning_rate)\u001b[0m\n\u001b[0;32m     30\u001b[0m mean, var \u001b[38;5;241m=\u001b[39m meanvar(batch, enc, training\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[0;32m     31\u001b[0m batch_ \u001b[38;5;241m=\u001b[39m reparameterize(mean, var)\n\u001b[1;32m---> 32\u001b[0m loss \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch_\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     33\u001b[0m epoch_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m loss\n\u001b[0;32m     34\u001b[0m num_batches \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\sayan\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\tensorflow\\python\\util\\traceback_utils.py:150\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    148\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    149\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 150\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    151\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    152\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[1;32mc:\\Users\\sayan\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py:915\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    912\u001b[0m compiler \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mxla\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnonXla\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    914\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m OptionalXlaContext(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile):\n\u001b[1;32m--> 915\u001b[0m   result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)\n\u001b[0;32m    917\u001b[0m new_tracing_count \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexperimental_get_tracing_count()\n\u001b[0;32m    918\u001b[0m without_tracing \u001b[38;5;241m=\u001b[39m (tracing_count \u001b[38;5;241m==\u001b[39m new_tracing_count)\n",
      "File \u001b[1;32mc:\\Users\\sayan\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py:947\u001b[0m, in \u001b[0;36mFunction._call\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    944\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock\u001b[38;5;241m.\u001b[39mrelease()\n\u001b[0;32m    945\u001b[0m   \u001b[38;5;66;03m# In this case we have created variables on the first call, so we run the\u001b[39;00m\n\u001b[0;32m    946\u001b[0m   \u001b[38;5;66;03m# defunned version which is guaranteed to never create variables.\u001b[39;00m\n\u001b[1;32m--> 947\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_stateless_fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)  \u001b[38;5;66;03m# pylint: disable=not-callable\u001b[39;00m\n\u001b[0;32m    948\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_stateful_fn \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    949\u001b[0m   \u001b[38;5;66;03m# Release the lock early so that multiple threads can perform the call\u001b[39;00m\n\u001b[0;32m    950\u001b[0m   \u001b[38;5;66;03m# in parallel.\u001b[39;00m\n\u001b[0;32m    951\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock\u001b[38;5;241m.\u001b[39mrelease()\n",
      "File \u001b[1;32mc:\\Users\\sayan\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\tensorflow\\python\\eager\\function.py:2496\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   2493\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock:\n\u001b[0;32m   2494\u001b[0m   (graph_function,\n\u001b[0;32m   2495\u001b[0m    filtered_flat_args) \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_maybe_define_function(args, kwargs)\n\u001b[1;32m-> 2496\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mgraph_function\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_flat\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   2497\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfiltered_flat_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcaptured_inputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgraph_function\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcaptured_inputs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\sayan\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\tensorflow\\python\\eager\\function.py:1862\u001b[0m, in \u001b[0;36mConcreteFunction._call_flat\u001b[1;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[0;32m   1858\u001b[0m possible_gradient_type \u001b[38;5;241m=\u001b[39m gradients_util\u001b[38;5;241m.\u001b[39mPossibleTapeGradientTypes(args)\n\u001b[0;32m   1859\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (possible_gradient_type \u001b[38;5;241m==\u001b[39m gradients_util\u001b[38;5;241m.\u001b[39mPOSSIBLE_GRADIENT_TYPES_NONE\n\u001b[0;32m   1860\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m executing_eagerly):\n\u001b[0;32m   1861\u001b[0m   \u001b[38;5;66;03m# No tape is watching; skip to running the function.\u001b[39;00m\n\u001b[1;32m-> 1862\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_build_call_outputs(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_inference_function\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1863\u001b[0m \u001b[43m      \u001b[49m\u001b[43mctx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcancellation_manager\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcancellation_manager\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[0;32m   1864\u001b[0m forward_backward \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_select_forward_and_backward_functions(\n\u001b[0;32m   1865\u001b[0m     args,\n\u001b[0;32m   1866\u001b[0m     possible_gradient_type,\n\u001b[0;32m   1867\u001b[0m     executing_eagerly)\n\u001b[0;32m   1868\u001b[0m forward_function, args_with_tangents \u001b[38;5;241m=\u001b[39m forward_backward\u001b[38;5;241m.\u001b[39mforward()\n",
      "File \u001b[1;32mc:\\Users\\sayan\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\tensorflow\\python\\eager\\function.py:499\u001b[0m, in \u001b[0;36m_EagerDefinedFunction.call\u001b[1;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[0;32m    497\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m _InterpolateFunctionError(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    498\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m cancellation_manager \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 499\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[43mexecute\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexecute\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    500\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msignature\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    501\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_num_outputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    502\u001b[0m \u001b[43m        \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    503\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattrs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    504\u001b[0m \u001b[43m        \u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mctx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    505\u001b[0m   \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    506\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m execute\u001b[38;5;241m.\u001b[39mexecute_with_cancellation(\n\u001b[0;32m    507\u001b[0m         \u001b[38;5;28mstr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msignature\u001b[38;5;241m.\u001b[39mname),\n\u001b[0;32m    508\u001b[0m         num_outputs\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_outputs,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    511\u001b[0m         ctx\u001b[38;5;241m=\u001b[39mctx,\n\u001b[0;32m    512\u001b[0m         cancellation_manager\u001b[38;5;241m=\u001b[39mcancellation_manager)\n",
      "File \u001b[1;32mc:\\Users\\sayan\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\tensorflow\\python\\eager\\execute.py:54\u001b[0m, in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     52\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m     53\u001b[0m   ctx\u001b[38;5;241m.\u001b[39mensure_initialized()\n\u001b[1;32m---> 54\u001b[0m   tensors \u001b[38;5;241m=\u001b[39m \u001b[43mpywrap_tfe\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mTFE_Py_Execute\u001b[49m\u001b[43m(\u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_handle\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mop_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     55\u001b[0m \u001b[43m                                      \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     56\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m core\u001b[38;5;241m.\u001b[39m_NotOkStatusException \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m     57\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "train_diffusion_model(diffusion_model, train_dataset, epochs=100000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "776f5882",
   "metadata": {},
   "outputs": [],
   "source": [
    "diffusion_model.save_weights('DiT.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f8849bc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff059bb2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4da683ba",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dc849ac",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e803f5f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f0ddb5a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "814cd861",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39b06335",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f39ffe47",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51bc0c03",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe317ab4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73d20fae",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9395d69",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c9d3675",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c511c5b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
